"""
Servi√ßo RAG (Retrieval-Augmented Generation) para consulta do PPC do curso.
Este servi√ßo permite fazer perguntas sobre o Projeto Pedag√≥gico do Curso usando IA.
"""

from __future__ import annotations
import os
import logging
from typing import Optional, Dict, Any
from pathlib import Path
from datetime import datetime
from agno.models.google import Gemini

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.knowledge.embedder.ollama import OllamaEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.lancedb import LanceDb, SearchType
from agno.models.huggingface import HuggingFace
from dotenv import load_dotenv
import time

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PPCChatbotService:
    """Servi√ßo de chatbot para consultas sobre o PPC do curso."""
    
    _instance: Optional['PPCChatbotService'] = None
    _agent: Optional[Agent] = None
    _initialized: bool = False
    
    def __new__(cls) -> 'PPCChatbotService':
        """Implementa padr√£o Singleton."""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        """Inicializa o servi√ßo (apenas uma vez)."""
        if not self._initialized:
            # Atributos de estado
            self.model: Optional[HuggingFace] = None
            self.embedder: Optional[OllamaEmbedder] = None
            self.vector_db: Optional[LanceDb] = None
            self.knowledge: Optional[Knowledge] = None
            self.db: Optional[SqliteDb] = None
            self._knowledge_loaded: bool = False
            self._initialized_at: Optional[datetime] = None
            self._last_question_at: Optional[datetime] = None
            self._last_latency: Optional[float] = None
            self._total_questions: int = 0
            self._setup_service()
            self._initialized = True
    
    def _setup_service(self) -> None:
        """Configura todos os componentes do servi√ßo RAG baseado no script funcional."""
        logger.info("=== CONFIGURANDO AGENTE RAG ===")
        
        # Carregar vari√°veis de ambiente
        load_dotenv(override=True)
        
        # Configurar caminhos
        self.db_url = "./data/lancedb"
        self.sqlite_db_path = "./data/ppc_chat.db"
        self.ppc_file_path = Path(__file__).resolve().parents[1] / "resources" / "PPC.pdf"
        
        # Criar diret√≥rios se n√£o existirem
        Path(self.db_url).parent.mkdir(parents=True, exist_ok=True)
        Path(self.sqlite_db_path).parent.mkdir(parents=True, exist_ok=True)
        
        try:
            self._setup_model()
            self._initialized_at = datetime.utcnow()
            logger.info("‚úÖ Agente configurado com sucesso!")
        except Exception as e:
            logger.error(f"‚ùå Erro ao inicializar servi√ßo: {e}")
            raise
    
    def _setup_model(self) -> None:
        """Configura o modelo seguindo exatamente o script funcional."""
        print("=== CONFIGURANDO AGENTE RAG ===")
        print("1. Configurando modelo de linguagem...")

        huggingface_api_key = os.getenv("HF_TOKEN")
        # Configurar Gemini com API key do .env
        google_api_key = os.getenv("GOOGLE_API_KEY")
        model = None
        if google_api_key:  
            try:
                print("   Tentando carregar modelo Gemini...")
                model = Gemini(
                    id="gemini-2.5-flash", 
                    api_key=google_api_key,
                )
                print("‚úÖ Modelo Gemini carregado com sucesso!")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Modelo Gemini n√£o dispon√≠vel: {str(e)[:80]}...")
        if model is None:
            if not huggingface_api_key:
                print("‚ùå HF_TOKEN n√£o encontrada no arquivo .env")
                raise RuntimeError("HF_TOKEN n√£o encontrada. Configure a vari√°vel de ambiente para usar os modelos HuggingFace.")
            else:
                print(f"‚úÖ HF_TOKEN carregada: {huggingface_api_key[:10]}...")
                
                # Lista de modelos para tentar (em ordem de prefer√™ncia)
                models_to_try = [
                    ("meta-llama/Llama-3.1-8B-Instruct:featherless-ai", "Llama 3.1 8B (Novita)"),
                    ("meta-llama/Meta-Llama-3-8B-Instruct:featherless-ai", "Meta Llama 3 8B (Featherless)"),
                    ("mistralai/Mistral-7B-Instruct-v0.2:featherless-ai", "Mistral 7B (Featherless)"),
                ]
                
                for model_id, model_name in models_to_try:
                    try:
                        print(f"   Tentando carregar {model_name}...")
                        hf_kwargs = {"api_key": huggingface_api_key}
                        provider_suffix = None

                        if ":" in model_id:
                            base_model_id, provider_suffix = model_id.split(":", 1)
                            hf_kwargs["id"] = base_model_id
                        else:
                            base_model_id = model_id
                            hf_kwargs["id"] = base_model_id

                        if provider_suffix:
                            existing_client_params = hf_kwargs.get("client_params") or {}
                            existing_client_params.update({"provider": provider_suffix})
                            hf_kwargs["client_params"] = existing_client_params

                        model = HuggingFace(**hf_kwargs)
                        print(f"‚úÖ {model_name} carregado com sucesso!")
                        break
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  {model_name} n√£o dispon√≠vel: {str(e)[:80]}...")
                        continue
                if model is None:
                    raise RuntimeError(
                        "Nenhum modelo HuggingFace p√¥de ser carregado. Verifique o HF_TOKEN ou o provedor configurado."
                    )

        self.model = model

        # Create Ollama embedder
        print("2. Configurando embedder...")
        embedder = OllamaEmbedder(
            id="nomic-embed-text", 
            dimensions=768,
        )

        self.embedder = embedder

        # Create the vector database
        print("3. Configurando banco de dados vetorial...")
        vector_db = LanceDb(
            table_name="recipes",
            uri=self.db_url,
            embedder=embedder,
            search_type=SearchType.hybrid,
        )

        self.vector_db = vector_db

        print("4. Configurando base de conhecimento...")
        knowledge = Knowledge(vector_db=vector_db, max_results=25)

        self.knowledge = knowledge

        # Verificar se o banco vetorial j√° possui dados
        vector_db_path = f"{self.db_url}/recipes.lance"
        has_existing_data = False

        if os.path.exists(vector_db_path):
            print("üìö Verificando se a base de conhecimento possui dados...")
            try:
                # Verificar se h√° documentos na tabela
                import lancedb
                db = lancedb.connect(self.db_url)
                table = db.open_table("recipes")
                doc_count = table.count_rows()
                
                if doc_count > 0:
                    print(f"‚úÖ Base de conhecimento encontrada com {doc_count} documentos!")
                    has_existing_data = True
                else:
                    print("‚ö†Ô∏è  Base de conhecimento existe mas est√° vazia")
                    has_existing_data = False
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  Erro ao verificar dados existentes: {e}")
                print("üîÑ Recarregando conte√∫do...")
                has_existing_data = False

        if not has_existing_data:
            print("üìö Carregando conte√∫do do PPC.pdf pela primeira vez...")
            print("   Isso pode demorar alguns minutos...")
            
            try:
                # Adicionando o arquivo PPC.pdf
                knowledge.add_content(
                    name="PPC Document",
                    path=str(self.ppc_file_path)
                )
                print("‚úÖ Conte√∫do do PPC.pdf adicionado com sucesso!")
                has_existing_data = True
            except Exception as e:
                print(f"‚ùå Erro ao carregar PPC.pdf: {e}")
                print("Continuando sem a base de conhecimento...")

        self._knowledge_loaded = has_existing_data

        print("5. Configurando banco de dados SQLite...")
        # Verificar se o banco SQLite j√° existe
        sqlite_exists = os.path.exists(self.sqlite_db_path)
        if sqlite_exists:
            print("üìä Banco de dados SQLite j√° existe, reutilizando...")
        else:
            print("üìä Criando novo banco de dados SQLite...")

        db = SqliteDb(db_file=self.sqlite_db_path)

        self.db = db

        print("6. Criando agente...")
        self._agent = Agent(
            session_id="rag_session", 
            user_id="user",  
            model=model,
            knowledge=knowledge,
            db=db,
        )

        print("‚úÖ Agente configurado com sucesso!")
        print("=" * 50)

    
    def ask_question(self, question: str) -> Dict[str, Any]:
        """Executa uma pergunta e retorna um payload estruturado para a interface."""
        if not self._agent:
            raise RuntimeError("Agente n√£o inicializado. Chame initialize() primeiro.")

        normalized_question = (question or "").strip()
        if not normalized_question:
            return {
                "success": False,
                "error": "Pergunta vazia. Por favor, digite uma pergunta sobre o PPC.",
            }

        try:
            logger.info("Pergunta recebida: %s", normalized_question[:150])
            start = time.perf_counter()
            response = self._agent.run(normalized_question)
            latency = time.perf_counter() - start

            answer_text = None
            if hasattr(response, "content") and response.content:
                answer_text = response.content
            elif hasattr(response, "output") and response.output:
                answer_text = response.output

            if answer_text is None:
                raise ValueError("O modelo n√£o retornou conte√∫do.")

            if isinstance(answer_text, list):
                # Alguns modelos retornam lista de fragmentos
                answer_text = "\n".join(str(part) for part in answer_text if part)

            answer_text = str(answer_text).strip()
            if not answer_text:
                raise ValueError("Resposta vazia gerada pelo modelo.")

            # Atualizar m√©tricas internas
            self._last_question_at = datetime.utcnow()
            self._last_latency = latency
            self._total_questions += 1

            logger.info("Resposta gerada em %.2fs", latency)
            return {
                "success": True,
                "answer": answer_text,
                "method": "agent",
                "latency": latency,
                "question": normalized_question,
            }

        except Exception as exc:
            logger.exception("Erro ao processar pergunta: %s", exc)
            return {
                "success": False,
                "error": str(exc),
            }
    
    def get_conversation_history(self, limit: int = 10) -> list:
        """
        Obt√©m hist√≥rico de conversas.
        
        Args:
            limit: N√∫mero m√°ximo de mensagens
            
        Returns:
            Lista de mensagens do hist√≥rico
        """
        try:
            if not self._agent or not self._agent.memory:
                return []
            
            # Obter hist√≥rico do agente
            messages = self._agent.memory.get_messages(limit=limit)
            
            return [
                {
                    "role": msg.role,
                    "content": msg.content,
                    "timestamp": getattr(msg, 'timestamp', None)
                }
                for msg in messages
            ]
            
        except Exception as e:
            logger.error(f"Erro ao obter hist√≥rico: {e}")
            return []
    
    def clear_conversation(self) -> bool:
        """
        Limpa o hist√≥rico de conversas.
        
        Returns:
            True se sucesso, False caso contr√°rio
        """
        try:
            if self._agent and self._agent.memory:
                self._agent.memory.clear()
                logger.info("Hist√≥rico de conversas limpo")
                return True
            return False
        except Exception as e:
            logger.error(f"Erro ao limpar hist√≥rico: {e}")
            return False
    
    def get_status(self) -> Dict[str, Any]:
        """
        Obt√©m status do servi√ßo.
        
        Returns:
            Dict com informa√ß√µes do status
        """
        return {
            "initialized": self._initialized,
            "initialized_at": self._initialized_at.isoformat() if self._initialized_at else None,
            "model_type": type(self.model).__name__ if self.model else None,
            "knowledge_loaded": bool(self._knowledge_loaded),
            "agent_ready": self._agent is not None,
            "db_path": getattr(self, "db_url", None),
            "ppc_file_exists": self.ppc_file_path.exists() if hasattr(self, "ppc_file_path") else False,
            "total_questions": self._total_questions,
            "last_question_at": self._last_question_at.isoformat() if self._last_question_at else None,
            "last_latency": self._last_latency,
        }


# Fun√ß√£o para obter a inst√¢ncia singleton do servi√ßo
def get_ppc_service() -> PPCChatbotService:
    """Obt√©m a inst√¢ncia singleton do servi√ßo PPC."""
    return PPCChatbotService()

