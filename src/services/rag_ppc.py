"""
Servi√ßo RAG (Retrieval-Augmented Generation) para consulta do PPC do curso.
Este servi√ßo permite fazer perguntas sobre o Projeto Pedag√≥gico do Curso usando IA.
"""

from __future__ import annotations
import os
import logging
import hashlib
import json
from typing import Optional, Dict, Any, List
from pathlib import Path
from datetime import datetime
from agno.models.google import Gemini

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.knowledge.embedder.ollama import OllamaEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.lancedb import LanceDb, SearchType
from agno.models.huggingface import HuggingFace
from agno.models.openai import OpenAILike
from agno.knowledge.embedder.google import GeminiEmbedder

from dotenv import load_dotenv
import time

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ChatbotService:
    """Servi√ßo de chatbot para consultas sobre o PPC do curso."""
    
    _instance: Optional['ChatbotService'] = None
    _agent: Optional[Agent] = None
    _initialized: bool = False
    
    def __new__(cls, persist_history: bool = True) -> 'ChatbotService':
        """Implementa padr√£o Singleton."""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, persist_history: bool = True):
        """
        Inicializa o servi√ßo (apenas uma vez).
        
        Args:
            persist_history: Se True, armazena hist√≥rico de conversas em SQLite.
                           Se False, usa apenas mem√≥ria RAM (mais r√°pido, sem persist√™ncia).
        """
        if not self._initialized:
            # Atributos de estado
            self.model: Optional[HuggingFace] = None
            self.embedder: Optional[OllamaEmbedder] = None
            self.vector_db: Optional[LanceDb] = None
            self.knowledge: Optional[Knowledge] = None
            self.db: Optional[SqliteDb] = None
            self.persist_history: bool = persist_history
            self._knowledge_loaded: bool = False
            self._initialized_at: Optional[datetime] = None
            self._last_question_at: Optional[datetime] = None
            self._last_latency: Optional[float] = None
            self._total_questions: int = 0
            self._setup_service()
            self._initialized = True
    
    def _find_document_files(self) -> List[Path]:
        """
        Encontra todos os documentos PDF na pasta resources.
        Usa vari√°vel de ambiente RAG_DOCUMENTS_DIR se configurada.
        Suporta detec√ß√£o autom√°tica de ambiente (local vs VM).
        """
        # Prioridade 1: Vari√°vel de ambiente configur√°vel (para produ√ß√£o)
        env_dir = os.getenv("RAG_DOCUMENTS_DIR")
        if env_dir and env_dir.strip():
            resource_dir = Path(env_dir.strip())
            if resource_dir.exists():
                pdf_files = list(resource_dir.glob("*.pdf"))
                if pdf_files:
                    logger.info(f"‚úÖ Documentos encontrados (via RAG_DOCUMENTS_DIR): {resource_dir}")
                    for pdf_file in pdf_files:
                        logger.info(f"   üìÑ {pdf_file.name}")
                    return pdf_files
                else:
                    logger.warning(f"‚ö†Ô∏è  RAG_DOCUMENTS_DIR existe mas n√£o cont√©m PDFs: {resource_dir}")
        
        # Prioridade 2: Busca padr√£o em poss√≠veis diret√≥rios
        resource_dirs = [
            Path.cwd() / "src" / "resources",
            Path(__file__).resolve().parents[2] / "src" / "resources",
            Path("/app/src/resources"),  # Container path
        ]
        
        for resource_dir in resource_dirs:
            if resource_dir.exists():
                pdf_files = list(resource_dir.glob("*.pdf"))
                if pdf_files:
                    logger.info(f"‚úÖ Documentos encontrados em: {resource_dir}")
                    for pdf_file in pdf_files:
                        logger.info(f"   üìÑ {pdf_file.name}")
                    return pdf_files
        
        # Fallback: se n√£o encontrar nenhum, usar PPC.pdf padr√£o
        default_path = Path(__file__).resolve().parents[2] / "src" / "resources" / "PPC.pdf"
        logger.warning(f"‚ö†Ô∏è  Nenhum PDF encontrado. Usando fallback: {default_path}")
        return [default_path]
    
    def _compute_documents_hash(self, document_files: List[Path]) -> str:
        """
        Calcula hash dos documentos para detectar mudan√ßas.
        Hash baseado em: nome do arquivo + tamanho + data de modifica√ß√£o
        """
        hash_data = []
        for doc_file in sorted(document_files, key=lambda x: x.name):
            if doc_file.exists():
                stat = doc_file.stat()
                hash_data.append(f"{doc_file.name}:{stat.st_size}:{stat.st_mtime}")
        
        combined = "|".join(hash_data)
        return hashlib.sha256(combined.encode()).hexdigest()
    
    def _should_reindex_documents(self, document_files: List[Path], cache_dir: Path) -> bool:
        """
        Verifica se os documentos precisam ser reindexados.
        Retorna True se houver mudan√ßas ou se o hash n√£o existir.
        """
        hash_file = cache_dir / "documents_hash.json"
        current_hash = self._compute_documents_hash(document_files)
        
        if not hash_file.exists():
            logger.info("üîÑ Hash de documentos n√£o encontrado. Indexa√ß√£o necess√°ria.")
            return True
        
        try:
            with open(hash_file, 'r') as f:
                cached_data = json.load(f)
                cached_hash = cached_data.get('hash', '')
                
            if cached_hash != current_hash:
                logger.info("üîÑ Documentos modificados detectados. Reindexa√ß√£o necess√°ria.")
                return True
            else:
                logger.info("‚úÖ Documentos n√£o modificados. Usando cache existente.")
                return False
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Erro ao ler hash de cache: {e}. For√ßando reindexa√ß√£o.")
            return True
    
    def _save_documents_hash(self, document_files: List[Path], cache_dir: Path):
        """Salva o hash atual dos documentos para futuras verifica√ß√µes."""
        hash_file = cache_dir / "documents_hash.json"
        current_hash = self._compute_documents_hash(document_files)
        
        try:
            with open(hash_file, 'w') as f:
                json.dump({
                    'hash': current_hash,
                    'timestamp': datetime.utcnow().isoformat(),
                    'documents': [str(f.name) for f in document_files]
                }, f, indent=2)
            logger.info(f"üíæ Hash de documentos salvo: {current_hash[:8]}...")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Erro ao salvar hash: {e}")
    
    def _setup_service(self) -> None:
        """Configura todos os componentes do servi√ßo RAG baseado no script funcional."""
        logger.info("=== CONFIGURANDO AGENTE RAG ===")
        
        # Carregar vari√°veis de ambiente
        load_dotenv(override=True)
        
        # Configurar caminhos - usar diret√≥rio de cache ou temp se ./data n√£o tiver permiss√µes
        data_dir = Path.home() / ".cache" / "fasitech" / "rag"
        data_dir.mkdir(parents=True, exist_ok=True)
        
        self.db_url = str(data_dir / "lancedb")
        self.sqlite_db_path = str(data_dir / "ppc_chat.db")
        
        # Localizar documentos PDF
        self.document_files = self._find_document_files()
        
        logger.info(f"üìÅ Usando diret√≥rio de dados: {data_dir}")
        logger.info(f"üìÑ Documentos encontrados: {[f.name for f in self.document_files]}")
        
        # Criar diret√≥rios se n√£o existirem
        Path(self.db_url).mkdir(parents=True, exist_ok=True)
        
        try:
            self._setup_model()
            self._initialized_at = datetime.utcnow()
            logger.info("‚úÖ Agente configurado com sucesso!")
        except Exception as e:
            logger.error(f"‚ùå Erro ao inicializar servi√ßo: {e}")
            raise
    
    def _setup_model(self) -> None:
        """Configura o modelo seguindo exatamente o script funcional."""
        print("=== CONFIGURANDO AGENTE RAG ===")
        print("1. Configurando modelo de linguagem...")

        # Carregar vari√°veis de ambiente
        huggingface_api_key = os.getenv("HF_TOKEN")
        google_api_key = os.getenv("GOOGLE_API_KEY")
        maritaca_api_key = os.getenv("MARITALK_API_KEY")

        model = None
        if google_api_key:  
            try:
                print("   Tentando carregar modelo Gemini...")
                model = Gemini(
                    id="gemini-2.5-flash", 
                    api_key=google_api_key,
                )
                print("‚úÖ Modelo Gemini carregado com sucesso!")
            except Exception as e:
                model = None
                print(f"   ‚ö†Ô∏è  Modelo Gemini n√£o dispon√≠vel: {str(e)[:80]}...")
        if maritaca_api_key and model is None:
            try:
                print("   Tentando carregar modelo Maritaca...")
                model = OpenAILike(
                        id="sabia-3",
                        name="Maritaca Sabia 3",
                        api_key=maritaca_api_key,
                        base_url="https://chat.maritaca.ai/api",
                        temperature=0 )
                print("‚úÖ Modelo Maritaca carregado com sucesso!")
            except Exception as e:
                model = None
                print(f"   ‚ö†Ô∏è  Modelo Maritaca n√£o dispon√≠vel: {str(e)[:80]}...")
        if model is None:
            if not huggingface_api_key:
                print("‚ùå HF_TOKEN n√£o encontrada no arquivo .env")
                raise RuntimeError("HF_TOKEN n√£o encontrada. Configure a vari√°vel de ambiente para usar os modelos HuggingFace.")
            else:
                print(f"‚úÖ HF_TOKEN carregada: {huggingface_api_key[:10]}...")
                
                # Lista de modelos para tentar (em ordem de prefer√™ncia)
                models_to_try = [
                    ("meta-llama/Llama-3.1-8B-Instruct:featherless-ai", "Llama 3.1 8B (Novita)"),
                    ("meta-llama/Meta-Llama-3-8B-Instruct:featherless-ai", "Meta Llama 3 8B (Featherless)"),
                    ("mistralai/Mistral-7B-Instruct-v0.2:featherless-ai", "Mistral 7B (Featherless)"),
                ]
                
                for model_id, model_name in models_to_try:
                    try:
                        print(f"   Tentando carregar {model_name}...")
                        hf_kwargs = {"api_key": huggingface_api_key}
                        provider_suffix = None

                        if ":" in model_id:
                            base_model_id, provider_suffix = model_id.split(":", 1)
                            hf_kwargs["id"] = base_model_id
                        else:
                            base_model_id = model_id
                            hf_kwargs["id"] = base_model_id

                        if provider_suffix:
                            existing_client_params = hf_kwargs.get("client_params") or {}
                            existing_client_params.update({"provider": provider_suffix})
                            hf_kwargs["client_params"] = existing_client_params

                        model = HuggingFace(**hf_kwargs)
                        print(f"‚úÖ {model_name} carregado com sucesso!")
                        break
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  {model_name} n√£o dispon√≠vel: {str(e)[:80]}...")
                        continue
                if model is None:
                    raise RuntimeError(
                        "Nenhum modelo HuggingFace p√¥de ser carregado. Verifique o HF_TOKEN ou o provedor configurado."
                    )

        self.model = model

        # Create Ollama embedder
        print("2. Configurando embedder...")
        # O host padr√£o √© localhost:11434, que funciona perfeitamente
        # j√° que Ollama est√° rodando no mesmo container
        embedder = OllamaEmbedder(
            id="nomic-embed-text", 
            dimensions=768
        )

        self.embedder = GeminiEmbedder(dimensions=768)

        # Create the vector database
        print("3. Configurando banco de dados vetorial...")
        vector_db = LanceDb(
            table_name="recipes",
            uri=self.db_url,
            embedder=self.embedder,
            search_type=SearchType.hybrid,
        )

        self.vector_db = vector_db

        print("4. Configurando base de conhecimento...")
        # Otimiza√ß√£o: reduzir de 15 para 10 resultados para melhor velocidade
        knowledge = Knowledge(vector_db=vector_db, max_results=20)

        self.knowledge = knowledge

        # Verificar se precisa reindexar documentos usando sistema de hash
        cache_dir = Path(self.db_url).parent
        should_reindex = self._should_reindex_documents(self.document_files, cache_dir)
        
        vector_db_path = f"{self.db_url}/recipes.lance"
        has_existing_data = False

        if os.path.exists(vector_db_path) and not should_reindex:
            print("üìö Verificando se a base de conhecimento possui dados...")
            try:
                # Verificar se h√° documentos na tabela
                import lancedb
                db = lancedb.connect(self.db_url)
                table = db.open_table("recipes")
                doc_count = table.count_rows()
                
                if doc_count > 0:
                    print(f"‚úÖ Base de conhecimento encontrada com {doc_count} documentos!")
                    has_existing_data = True
                else:
                    print("‚ö†Ô∏è  Base de conhecimento existe mas est√° vazia")
                    has_existing_data = False
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  Erro ao verificar dados existentes: {e}")
                print("üîÑ Recarregando conte√∫do...")
                has_existing_data = False
        elif should_reindex:
            print("üîÑ Documentos modificados ou novos detectados. Reindexa√ß√£o necess√°ria.")
            has_existing_data = False

        if not has_existing_data:
            print("üìö Carregando/Reindexando documentos...")
            print("   Isso pode demorar alguns minutos...")
            
            try:
                # Verificar se pelo menos um arquivo existe
                existing_files = [f for f in self.document_files if f.exists()]
                if not existing_files:
                    raise FileNotFoundError(
                        f"Nenhum documento encontrado nos caminhos: {[str(f) for f in self.document_files]}\n"
                        f"Cwd: {Path.cwd()}\n"
                        f"File module dir: {Path(__file__).resolve().parent}"
                    )
                
                # Adicionando todos os documentos encontrados
                for doc_file in existing_files:
                    knowledge.add_content(
                        name=f"{doc_file.stem} Document",
                        path=str(doc_file)
                    )
                    print(f"‚úÖ Documento {doc_file.name} adicionado com sucesso!")
                
                # Salvar hash dos documentos ap√≥s indexa√ß√£o bem-sucedida
                self._save_documents_hash(existing_files, cache_dir)
                has_existing_data = True
            except FileNotFoundError as fe:
                print(f"‚ùå ERRO: {fe}")
                logger.error(f"Arquivo PPC n√£o encontrado: {fe}")
                print("‚ö†Ô∏è  Continuando sem a base de conhecimento...")
            except Exception as e:
                print(f"‚ùå Erro ao carregar documentos: {e}")
                logger.error(f"Erro ao carregar documentos: {e}")
                print("‚ö†Ô∏è  Continuando sem a base de conhecimento...")

        self._knowledge_loaded = has_existing_data

        # 5. Configurar SQLite apenas se persist_history=True
        db = None
        if self.persist_history:
            print("5. Configurando banco de dados SQLite (hist√≥rico persistente)...")
            sqlite_exists = os.path.exists(self.sqlite_db_path)
            if sqlite_exists:
                print("üìä Banco de dados SQLite j√° existe, reutilizando...")
            else:
                print("üìä Criando novo banco de dados SQLite...")
            db = SqliteDb(db_file=self.sqlite_db_path)
            self.db = db
        else:
            print("5. SQLite desabilitado (persist_history=False). Usando apenas mem√≥ria RAM.")
            self.db = None

        print("6. Criando agente...")
        self._agent = Agent(
            session_id="rag_session", 
            user_id="user",  
            model=model,
            knowledge=knowledge,
            db=db,  # Pode ser None se persist_history=False
        )

        print("‚úÖ Agente configurado com sucesso!")
        print("=" * 50)

    
    def _post_process_answer(self, answer_text: str) -> str:
        """
        P√≥s-processamento da resposta do modelo.
        Remove markdown redundante e normaliza formata√ß√£o.
        """
        # Remover m√∫ltiplas linhas em branco
        import re
        answer_text = re.sub(r'\n{3,}', '\n\n', answer_text)
        
        # Remover espa√ßos extras no in√≠cio/fim de linhas
        lines = [line.rstrip() for line in answer_text.split('\n')]
        answer_text = '\n'.join(lines)
        
        # Normalizar cita√ß√µes de markdown excessivas
        answer_text = re.sub(r'```markdown\n?(.*?)\n?```', r'\1', answer_text, flags=re.DOTALL)
        
        return answer_text.strip()
    
    def _extract_sources(self, response: Any) -> List[str]:
        """
        Extrai fontes/documentos utilizados na resposta.
        """
        sources = []
        
        # Tentar extrair de diferentes estruturas de resposta
        if hasattr(response, 'documents') and response.documents:
            for doc in response.documents:
                if hasattr(doc, 'name') and doc.name:
                    sources.append(doc.name)
                elif hasattr(doc, 'metadata') and doc.metadata:
                    source = doc.metadata.get('source', doc.metadata.get('name', ''))
                    if source:
                        sources.append(source)
        
        # Remover duplicatas mantendo ordem
        seen = set()
        unique_sources = []
        for source in sources:
            if source not in seen:
                seen.add(source)
                unique_sources.append(source)
        
        return unique_sources
    
    def ask_question(self, question: str, stream: bool = False) -> Dict[str, Any]:
        """
        Executa uma pergunta e retorna um payload estruturado para a interface.
        
        Args:
            question: Pergunta do usu√°rio
            stream: Se True, habilita streaming (futuro suporte)
            
        Returns:
            Dict com resposta, lat√™ncia, fontes e metadados
        """
        if not self._agent:
            raise RuntimeError("Agente n√£o inicializado. Chame initialize() primeiro.")

        normalized_question = (question or "").strip()
        if not normalized_question:
            return {
                "success": False,
                "error": "Pergunta vazia. Por favor, digite uma pergunta sobre o PPC.",
            }

        try:
            logger.info("Pergunta recebida: %s", normalized_question[:150])
            start = time.perf_counter()
            
            # Executar pergunta no agente
            response = self._agent.run(normalized_question, stream=stream)
            latency = time.perf_counter() - start

            # Extrair resposta de texto
            answer_text = None
            if hasattr(response, "content") and response.content:
                answer_text = response.content
            elif hasattr(response, "output") and response.output:
                answer_text = response.output

            if answer_text is None:
                raise ValueError("O modelo n√£o retornou conte√∫do.")

            if isinstance(answer_text, list):
                # Alguns modelos retornam lista de fragmentos
                answer_text = "\n".join(str(part) for part in answer_text if part)

            answer_text = str(answer_text).strip()
            if not answer_text:
                raise ValueError("Resposta vazia gerada pelo modelo.")

            # P√≥s-processamento da resposta
            answer_text = self._post_process_answer(answer_text)
            
            # Extrair fontes utilizadas
            sources = self._extract_sources(response)

            # Atualizar m√©tricas internas
            self._last_question_at = datetime.utcnow()
            self._last_latency = latency
            self._total_questions += 1

            logger.info("Resposta gerada em %.2fs (processamento inclu√≠do)", latency)
            
            result = {
                "success": True,
                "answer": answer_text,
                "method": "agent",
                "latency": latency,
                "question": normalized_question,
            }
            
            # Adicionar fontes se encontradas
            if sources:
                result["sources"] = sources
                logger.info(f"Fontes utilizadas: {', '.join(sources)}")
            
            return result

        except Exception as exc:
            logger.exception("Erro ao processar pergunta: %s", exc)
            return {
                "success": False,
                "error": str(exc),
            }
    
    def get_conversation_history(self, limit: int = 10) -> list:
        """
        Obt√©m hist√≥rico de conversas.
        
        Args:
            limit: N√∫mero m√°ximo de mensagens
            
        Returns:
            Lista de mensagens do hist√≥rico
        """
        try:
            if not self._agent or not self._agent.memory:
                return []
            
            # Obter hist√≥rico do agente
            messages = self._agent.memory.get_messages(limit=limit)
            
            return [
                {
                    "role": msg.role,
                    "content": msg.content,
                    "timestamp": getattr(msg, 'timestamp', None)
                }
                for msg in messages
            ]
            
        except Exception as e:
            logger.error(f"Erro ao obter hist√≥rico: {e}")
            return []
    
    def clear_conversation(self) -> bool:
        """
        Limpa o hist√≥rico de conversas.
        
        Returns:
            True se sucesso, False caso contr√°rio
        """
        try:
            if self._agent and self._agent.memory:
                self._agent.memory.clear()
                logger.info("Hist√≥rico de conversas limpo")
                return True
            return False
        except Exception as e:
            logger.error(f"Erro ao limpar hist√≥rico: {e}")
            return False
    
    def get_status(self) -> Dict[str, Any]:
        """
        Obt√©m status do servi√ßo.
        
        Returns:
            Dict com informa√ß√µes do status e configura√ß√µes
        """
        return {
            "initialized": self._initialized,
            "initialized_at": self._initialized_at.isoformat() if self._initialized_at else None,
            "model_type": type(self.model).__name__ if self.model else None,
            "knowledge_loaded": bool(self._knowledge_loaded),
            "agent_ready": self._agent is not None,
            "db_path": getattr(self, "db_url", None),
            "persist_history": self.persist_history,
            "sqlite_enabled": self.db is not None,
            "max_results": self.knowledge.max_results if self.knowledge else None,
            "document_files": [f.name for f in self.document_files] if hasattr(self, "document_files") else [],
            "documents_exist": any(f.exists() for f in self.document_files) if hasattr(self, "document_files") else False,
            "total_questions": self._total_questions,
            "last_question_at": self._last_question_at.isoformat() if self._last_question_at else None,
            "last_latency": self._last_latency,
            "avg_latency": self._last_latency / max(self._total_questions, 1) if self._last_latency else None,
        }


# Fun√ß√£o para obter a inst√¢ncia singleton do servi√ßo
def get_service(persist_history: bool = True) -> ChatbotService:
    """
    Obt√©m a inst√¢ncia singleton do servi√ßo PPC.
    
    Args:
        persist_history: Se True, mant√©m hist√≥rico em SQLite (padr√£o).
                        Se False, usa apenas RAM (mais r√°pido).
    """
    return ChatbotService(persist_history=persist_history)


# Alias para compatibilidade com c√≥digo legado
PPCChatbotService = ChatbotService

